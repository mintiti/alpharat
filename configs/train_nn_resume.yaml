# MLP training - resume from previous checkpoint
# Use this template when continuing training on new data
# Usage: uv run python scripts/train.py configs/train_nn_resume.yaml --device mps
#
# Before running:
#   1. Set `name` to your run name (e.g., mlp_v2)
#   2. Set `data.train_dir` and `data.val_dir` to your shard paths
#   3. Set `resume_from` to the checkpoint you're continuing from

name: <EXPERIMENT_NAME>  # e.g., mlp_v2, symmetric_iter3

model:
  architecture: mlp
  hidden_dim: 256
  dropout: 0.0
  p_augment: 0.5

optim:
  architecture: mlp
  lr: 1e-3
  policy_weight: 1.0
  value_weight: 1.0

data:
  # Path format: experiments/shards/{group}/{uuid}/train
  train_dir: experiments/shards/<GROUP>/<UUID>/train
  val_dir: experiments/shards/<GROUP>/<UUID>/val

seed: 42

# Checkpoint to resume from (weights loaded, training continues)
# Example: experiments/runs/mlp_v1/checkpoints/best_model.pt
resume_from: <PARENT_CHECKPOINT_PATH>
