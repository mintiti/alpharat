# Training configuration
# Usage: uv run python scripts/train.py configs/train.yaml

model:
  hidden_dim: 256
  dropout: 0.0

optim:
  lr: 1e-3
  policy_weight: 1.0
  value_weight: 1.0
  # Nash consistency loss: enforces game-theoretic consistency between payout and policy
  # - nash_weight: loss weight (0 = disabled)
  # - nash_mode: "target" uses MCTS policies, "predicted" uses NN's own policies
  nash_weight: 0.0
  nash_mode: target
  p_augment: 0.5      # probability of player swap augmentation per sample

data:
  train_dir: data/940b6e74-01c1-4a2e-ac6d-9ca2c1ec54c5/train
  val_dir: data/940b6e74-01c1-4a2e-ac6d-9ca2c1ec54c5/val

seed: 42
