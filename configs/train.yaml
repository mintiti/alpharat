# MLP training configuration
# Usage: uv run python scripts/train.py configs/train.yaml
#
# Before running:
#   1. Set `name` to your run name (e.g., mlp_v1)
#   2. Set `data.train_dir` and `data.val_dir` to your shard paths
#      (run `ls experiments/shards/` to see available shards)

name: <EXPERIMENT_NAME>  # e.g., mlp_v1, symmetric_5x5

model:
  architecture: mlp
  hidden_dim: 256
  dropout: 0.0
  p_augment: 0.5  # probability of player swap augmentation per sample

optim:
  architecture: mlp
  lr: 1e-3
  policy_weight: 1.0
  value_weight: 1.0
  # Nash consistency loss: enforces game-theoretic consistency between payout and policy
  # - nash_weight: loss weight (0 = disabled)
  # - nash_mode: "target" uses MCTS policies, "predicted" uses NN's own policies
  nash_weight: 0.0
  nash_mode: target

data:
  # Path format: experiments/shards/{group}/{uuid}/train
  # Example: experiments/shards/5x5_uniform/a1b2c3d4-.../train
  train_dir: experiments/shards/<GROUP>/<UUID>/train
  val_dir: experiments/shards/<GROUP>/<UUID>/val

seed: 42
