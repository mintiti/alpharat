# MLP training configuration
# Usage: alpharat-train configs/train.yaml --name mlp_v1 --shards GROUP/UUID
#
# CLI overrides (merged into frozen config for reproducibility):
#   --name NAME         Override experiment name
#   --shards GROUP/UUID Override data paths (e.g., 'mygroup/abc123')
#
# To find shard IDs: ls experiments/shards/

name: default  # Override with --name

model:
  architecture: mlp
  hidden_dim: 256
  dropout: 0.0
  p_augment: 0.5  # probability of player swap augmentation per sample

optim:
  architecture: mlp
  lr: 1e-3
  policy_weight: 1.0
  value_weight: 1.0
  # Nash consistency loss: enforces game-theoretic consistency between payout and policy
  # - nash_weight: loss weight (0 = disabled)
  # - nash_mode: "target" uses MCTS policies, "predicted" uses NN's own policies
  nash_weight: 0.0
  nash_mode: target

data:
  # Override with --shards, or set explicitly:
  train_dir: experiments/shards/GROUP/UUID/train
  val_dir: experiments/shards/GROUP/UUID/val

seed: 42
